{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Ingredients extraction\"\n",
    "execute:\n",
    "  freeze: true\n",
    "title-block-banner: \"#497D74\"\n",
    "description: Explore possibilities for ingredients extraction from a given input text.\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    number-sections: false\n",
    "    toc: true\n",
    "    toc-location: right\n",
    "    toc-depth: 2\n",
    "    toc-expand: 1\n",
    "    callout-icon: true\n",
    "    highlight-style: tango\n",
    "    code-line-numbers: ayu\n",
    "    embed-resources: true\n",
    "    theme: flatly\n",
    "    grid:\n",
    "        body-width: 1000px\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "We will add a feature to handle a recipe requests from the user by using natural language. \n",
    "The request sentence will be parsed by a LLM to extract the ingredients that the user wants to include or exclude \n",
    "from the recipe. This list will then be used as input of the recipe recommendation system. The goal of this notebook \n",
    "is to explore the performance of LLMs for this task.\n",
    "\n",
    "## Test dataset\n",
    "\n",
    "We will use a dataset containing 20 recipe requests. For each examples it contains:\n",
    "\n",
    "- A sentence in which the user ask for a recipe.\n",
    "- The list of ingredients that:\n",
    "  - The user likes\n",
    "  - The user doesn't like\n",
    "\n",
    "This will be used as a benchmark. We will see if the model can accurately extract the ingredients from the sentence.\n",
    "\n",
    "Here is an example of recipe request and the corresponding expected ingredients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "from great_tables import GT\n",
    "from IPython.display import Image\n",
    "from IPython.display import display as idisplay\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "TEST_FILE_PATH = Path.cwd().parent / \"data\" / \"test_cases_request_to_ingredients.json\"\n",
    "\n",
    "\n",
    "class RecipeRequest(BaseModel):\n",
    "    \"\"\"A request for a recipe recommendation.\"\"\"\n",
    "\n",
    "    recipe_request: str\n",
    "    positive_ingredients: list[str]\n",
    "    negative_ingredients: list[str]\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "with Path.open(TEST_FILE_PATH, encoding=\"utf-8\") as json_file:\n",
    "    _test_recipes_requests_ingredients = json.load(json_file)\n",
    "\n",
    "test_recipes_requests_ingredients = {k: RecipeRequest(**v) for k, v in _test_recipes_requests_ingredients.items()}\n",
    "\n",
    "# Display a sample\n",
    "for i, k in enumerate(test_recipes_requests_ingredients.keys()):\n",
    "    if i >= 2:\n",
    "        break\n",
    "    print(test_recipes_requests_ingredients[k].model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "\n",
    "In order to easily compare models locally, we will use ollama and openai sdk to use different llms. \n",
    "Ollama supports the structured output which allows to handle the LLM's output easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: false\n",
    "\n",
    "class Ingredients(BaseModel):\n",
    "    \"\"\"Lists of ingredients associated to positive and negative feelings\"\"\"\n",
    "\n",
    "    positive_ingredients: list[str]\n",
    "    negative_ingredients: list[str]\n",
    "\n",
    "\n",
    "def get_ingredients(recipe_request: str, llm_credentials: dict, llm_model: str) -> Ingredients:\n",
    "    \"\"\"Get the list of positive and negative ingredients for a given recipe request.\"\"\"\n",
    "\n",
    "    client = OpenAI(**llm_credentials)\n",
    "\n",
    "    try:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            temperature=0,\n",
    "            model=llm_model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": (\n",
    "                        \"Provide the list of positive and negative ingredients for the following recipe \"\n",
    "                        f\"request in lowercase: {recipe_request}\"\n",
    "                    ),\n",
    "                }\n",
    "            ],\n",
    "            response_format=Ingredients,\n",
    "        )\n",
    "\n",
    "        recipe_response = completion.choices[0].message\n",
    "        if recipe_response.parsed:  # noqa: SIM108\n",
    "            ingredients = recipe_response.parsed\n",
    "        else:\n",
    "            ingredients = Ingredients(positive_ingredients=[], negative_ingredients=[])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        ingredients = Ingredients(positive_ingredients=[], negative_ingredients=[])\n",
    "\n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(\n",
    "    test_dataset: dict[str, RecipeRequest], llm_credentials: dict, llm_model: str\n",
    ") -> dict[str, dict[str, Any]]:\n",
    "    \"\"\"Test a list of ingredients with a given LLM.\"\"\"\n",
    "    benchmark_results = {}\n",
    "    for _id, req in test_dataset.items():\n",
    "        computed_result = get_ingredients(\n",
    "            recipe_request=req.recipe_request,\n",
    "            llm_credentials=llm_credentials,\n",
    "            llm_model=llm_model,\n",
    "        )\n",
    "        expected_result = Ingredients(\n",
    "            positive_ingredients=req.positive_ingredients, negative_ingredients=req.negative_ingredients\n",
    "        )\n",
    "\n",
    "        benchmark_results[_id] = {\n",
    "            \"recipe_request\": req.recipe_request,\n",
    "            \"computed_result\": computed_result,\n",
    "            \"expected_result\": expected_result,\n",
    "            \"correct_inference\": computed_result == expected_result,\n",
    "        }\n",
    "\n",
    "    return benchmark_results\n",
    "\n",
    "\n",
    "def benchmark_overview(benchmark_results: dict[str, dict[str, Any]]) -> None:\n",
    "    \"\"\"Generate a table and a dataframe allowing to display the benchmark results.\"\"\"\n",
    "    NB_DISPLAYED_SAMPLES = 10\n",
    "\n",
    "    df_result = pd.DataFrame(benchmark_results).T\n",
    "    df_result[\"short_id\"] = df_result.index.str[:8]\n",
    "    table_result = (\n",
    "        GT(df_result.head(NB_DISPLAYED_SAMPLES), rowname_col=\"short_id\")\n",
    "        .tab_header(\n",
    "            title=\"Recipes ingredients extraction overview\",\n",
    "            subtitle=f\"Accuracy on full dataset: {df_result.correct_inference.mean():.1%}\",\n",
    "        )\n",
    "        .tab_source_note(source_note=f\"Only the first {NB_DISPLAYED_SAMPLES} cases are displayed.\")\n",
    "        .tab_options(\n",
    "            heading_background_color=\"#36665e\",\n",
    "            column_labels_background_color=\"#479487\",\n",
    "        )\n",
    "        .cols_label(recipe_request=\"Request\", computed_result=\"Computed ingredients\", correct_inference=\"Correct\")\n",
    "        .cols_hide(\"expected_result\")\n",
    "        .fmt(lambda x: \"✅\" if x else \"❌\", columns=[\"correct_inference\"])\n",
    "        .opt_align_table_header()\n",
    "        .cols_align(align=\"center\", columns=[\"recipe_request\", \"computed_result\"])\n",
    "        .opt_table_outline()\n",
    "    )\n",
    "\n",
    "    return df_result, table_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the benchmark against a list of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: false\n",
    "\n",
    "\n",
    "LLM_CREDENTIALS = {\"base_url\": \"http://localhost:11434/v1\", \"api_key\": \"ollama\"}\n",
    "AVAILABLE_MODELS = [\"smollm2:135m\", \"smollm2:360m\", \"llama3.2:1b\", \"gemma2:2b\", \"mistral:latest\"]\n",
    "\n",
    "models_result = {}\n",
    "for model in AVAILABLE_MODELS:\n",
    "    benchmark_results = benchmark(\n",
    "        test_dataset=test_recipes_requests_ingredients,\n",
    "        llm_credentials=LLM_CREDENTIALS,\n",
    "        llm_model=model,\n",
    "    )\n",
    "    df_result, table_result = benchmark_overview(benchmark_results)\n",
    "\n",
    "    models_result[model] = (df_result, table_result)\n",
    "\n",
    "    print(f\"Model: {model:<15} | Accuracy: {df_result.correct_inference.mean():.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two models have a  100% accuracy: `gemma2:2b` and `mistral:latest` (7b). \n",
    "Gemma2 is a smaller model than Mistral, and have a a good inference speed. \n",
    "We will therefore use this model in the recipes inference app.\n",
    "\n",
    "\n",
    "_Remark: Using `mistral:latest` instead of a frozen version of a model (e.g. `mistral:v0.3`) can be seen as a bad \n",
    "practice as this could limit reproducibility. This is not a big issue here as we don't decide to use this model for \n",
    "the app._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USED_MODEL = \"gemma2:2b\"\n",
    "models_result[USED_MODEL][1].tab_header(f\"[ {USED_MODEL} ] Recipes ingredients extraction overview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting\n",
    "\n",
    "The ingredients are sometimes in plural form and sometimes in singular form. We need to standardize them to singular \n",
    "form to be consistent with the recipe database format. \n",
    "We can take advantage of the versatility of the LLM to do this.\n",
    "After extracting the ingredients, we can ask the LLM to convert each item in singular form. \n",
    "\n",
    "\n",
    "To implement this workflow, we can use the library `LangGraph` from the `LangChain` ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"mistral:latest\", temperature=0)\n",
    "\n",
    "\n",
    "class IngredientsSimpleList(BaseModel):\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    recipe_request: str\n",
    "    ingredients: Ingredients\n",
    "    positive_ingredients_singular: list[str]\n",
    "    negative_ingredients_singular: list[str]\n",
    "    ingredients_singular: Ingredients\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def extract_ingredients(state: State) -> dict[str, str]:\n",
    "    \"\"\"First LLM call to generate extract positive ingredients\"\"\"\n",
    "\n",
    "    msg = llm.with_structured_output(Ingredients).invoke(\n",
    "        \"Provide the list of positive and negative ingredients for the following recipe \"\n",
    "        f\"request in lowercase: {state['recipe_request']}\"\n",
    "    )\n",
    "    return {\"ingredients\": msg}\n",
    "\n",
    "\n",
    "def make_positive_ingredients_singular(state: State) -> dict[str, list[str]]:\n",
    "    \"\"\"Second LLM call make positive ingredients singular\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Transform each item in the following list into its singular form.\n",
    "Make sure that no item is removed from the list.\n",
    "Here is the definition of the expected output: {IngredientsSimpleList.model_json_schema()}\n",
    "\n",
    "Here is the list: {state[\"ingredients\"].positive_ingredients}\"\"\"\n",
    "    msg = llm.with_structured_output(IngredientsSimpleList).invoke(prompt)\n",
    "    return {\"positive_ingredients_singular\": msg.ingredients}\n",
    "\n",
    "\n",
    "def make_negative_ingredients_singular(state: State) -> dict[str, list[str]]:\n",
    "    \"\"\"Third LLM call make negative ingredients singular\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Transform each item in the following list into its singular form.\n",
    "Make sure that no item is removed from the list.\n",
    "Here is the definition of the expected output: {IngredientsSimpleList.model_json_schema()}\n",
    "\n",
    "Here is the list: {state[\"ingredients\"].negative_ingredients}\"\"\"\n",
    "    msg = llm.with_structured_output(IngredientsSimpleList).invoke(prompt)\n",
    "    return {\"negative_ingredients_singular\": msg.ingredients}\n",
    "\n",
    "\n",
    "def make_ingredients_singular(state: State) -> dict[str, Ingredients]:\n",
    "    \"\"\"Gather previous results\"\"\"\n",
    "\n",
    "    ingredients_singular = Ingredients(\n",
    "        positive_ingredients=state[\"positive_ingredients_singular\"],\n",
    "        negative_ingredients=state[\"negative_ingredients_singular\"],\n",
    "    )\n",
    "    return {\"ingredients_singular\": ingredients_singular}\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"extract_ingredients\", extract_ingredients)\n",
    "workflow.add_node(\"make_positive_ingredients_singular\", make_positive_ingredients_singular)\n",
    "workflow.add_node(\"make_negative_ingredients_singular\", make_negative_ingredients_singular)\n",
    "workflow.add_node(\"make_ingredients_singular\", make_ingredients_singular)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "workflow.add_edge(START, \"extract_ingredients\")\n",
    "workflow.add_edge(\"extract_ingredients\", \"make_positive_ingredients_singular\")\n",
    "workflow.add_edge(\"extract_ingredients\", \"make_negative_ingredients_singular\")\n",
    "workflow.add_edge(\"make_positive_ingredients_singular\", \"make_ingredients_singular\")\n",
    "workflow.add_edge(\"make_negative_ingredients_singular\", \"make_ingredients_singular\")\n",
    "workflow.add_edge(\"make_ingredients_singular\", END)\n",
    "\n",
    "# Compile\n",
    "chain = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of library like `LangChain` comes with additionnal functionnalities, like the he ability to visualize the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show workflow\n",
    "idisplay(Image(chain.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also keeps track of every state of the workflow, which can be useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke\n",
    "recipe_request = \"What can I cook tonight? I'm a fan of eggs, milk, and flour, but I prefer not to include nuts.\"\n",
    "state = chain.invoke({\"recipe_request\": recipe_request})\n",
    "pprint(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While using this library can be beneficial, it introduces certain overheads. It adds an additional dependency to our project and increases complexity, potentially limiting compatibility and readability. For example, some models from `Ollama` are not compatible with structured outputs when accessed through the `LangChain` API.\n",
    "\n",
    "## Improvements\n",
    "\n",
    "The current method struggles with imprecise ingredient descriptions. For example, if a user mentions disliking \"meat,\" this general term won't match specific entries like \"beef\" or \"chicken\" in the recipe database. Similarly, a preference against \"spicy food\" won't align with specific spicy ingredients. To address this, we could implement a semantic search to identify the closest matching ingredient in the database and substitute the original term."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
