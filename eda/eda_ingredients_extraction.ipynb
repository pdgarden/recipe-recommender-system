{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Ingredients extraction\"\n",
    "execute:\n",
    "  freeze: true\n",
    "title-block-banner: \"#497D74\"\n",
    "description: Explore possibilities for ingredients extraction from a given input text.\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    number-sections: false\n",
    "    toc: true\n",
    "    toc-location: right\n",
    "    toc-depth: 2\n",
    "    toc-expand: 1\n",
    "    callout-icon: true\n",
    "    highlight-style: tango\n",
    "    code-line-numbers: ayu\n",
    "    embed-resources: true\n",
    "    theme: flatly\n",
    "    grid:\n",
    "        body-width: 1000px\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "We will add a feature to handle a recipe requests from the user by using natural language. \n",
    "The request sentence will be parsed by a LLM to extract the ingredients that the user wants to include or exclude \n",
    "from the recipe. This list will then be used as input of the recipe recommendation system. The goal of this notebook \n",
    "is to explore the performance of LLMs for this task.\n",
    "\n",
    "## Test dataset\n",
    "\n",
    "We will use a dataset containing 20 recipe requests. For each examples it contains:\n",
    "\n",
    "- A sentence in which the user ask for a recipe.\n",
    "- The list of ingredients that:\n",
    "  - The user likes\n",
    "  - The user doesn't like\n",
    "\n",
    "This will be used as a benchmark. We will see if the model can accurately extract the ingredients from the sentence.\n",
    "\n",
    "Here is an example of recipe request and the corresponding expected ingredients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "from great_tables import GT\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "TEST_FILE_PATH = Path.cwd().parent / \"data\" / \"test_cases_request_to_ingredients.json\"\n",
    "\n",
    "\n",
    "class RecipeRequest(BaseModel):\n",
    "    \"\"\"A request for a recipe recommendation.\"\"\"\n",
    "\n",
    "    recipe_request: str\n",
    "    positive_ingredients: list[str]\n",
    "    negative_ingredients: list[str]\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "with Path.open(TEST_FILE_PATH, encoding=\"utf-8\") as json_file:\n",
    "    _test_recipes_requests_ingredients = json.load(json_file)\n",
    "\n",
    "test_recipes_requests_ingredients = {k: RecipeRequest(**v) for k, v in _test_recipes_requests_ingredients.items()}\n",
    "\n",
    "# Display a sample\n",
    "for i, k in enumerate(test_recipes_requests_ingredients.keys()):\n",
    "    if i >= 2:\n",
    "        break\n",
    "    print(test_recipes_requests_ingredients[k].model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "\n",
    "In order to easily compare models locally, we will use ollama and openai sdk to use different llms. \n",
    "Ollama supports the structured output which allows to handle the LLM's output easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: false\n",
    "\n",
    "class Ingredients(BaseModel):\n",
    "    \"\"\"Lists of ingredients associated to positive and negative feelings\"\"\"\n",
    "\n",
    "    positive_ingredients: list[str]\n",
    "    negative_ingredients: list[str]\n",
    "\n",
    "\n",
    "def get_ingredients(recipe_request: str, llm_credentials: dict, llm_model: str) -> Ingredients:\n",
    "    \"\"\"Get the list of positive and negative ingredients for a given recipe request.\"\"\"\n",
    "\n",
    "    client = OpenAI(**llm_credentials)\n",
    "\n",
    "    try:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            temperature=0,\n",
    "            model=llm_model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": (\n",
    "                        \"Provide the list of positive and negative ingredients for the following recipe \"\n",
    "                        f\"request in lowercase: {recipe_request}\"\n",
    "                    ),\n",
    "                }\n",
    "            ],\n",
    "            response_format=Ingredients,\n",
    "        )\n",
    "\n",
    "        recipe_response = completion.choices[0].message\n",
    "        if recipe_response.parsed:  # noqa: SIM108\n",
    "            ingredients = recipe_response.parsed\n",
    "        else:\n",
    "            ingredients = Ingredients(positive_ingredients=[], negative_ingredients=[])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        ingredients = Ingredients(positive_ingredients=[], negative_ingredients=[])\n",
    "\n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(\n",
    "    test_dataset: dict[str, RecipeRequest], llm_credentials: dict, llm_model: str\n",
    ") -> dict[str, dict[str, Any]]:\n",
    "    \"\"\"Test a list of ingredients with a given LLM.\"\"\"\n",
    "    benchmark_results = {}\n",
    "    for _id, req in test_dataset.items():\n",
    "        computed_result = get_ingredients(\n",
    "            recipe_request=req.recipe_request,\n",
    "            llm_credentials=llm_credentials,\n",
    "            llm_model=llm_model,\n",
    "        )\n",
    "        expected_result = Ingredients(\n",
    "            positive_ingredients=req.positive_ingredients, negative_ingredients=req.negative_ingredients\n",
    "        )\n",
    "\n",
    "        benchmark_results[_id] = {\n",
    "            \"recipe_request\": req.recipe_request,\n",
    "            \"computed_result\": computed_result,\n",
    "            \"expected_result\": expected_result,\n",
    "            \"correct_inference\": computed_result == expected_result,\n",
    "        }\n",
    "\n",
    "    return benchmark_results\n",
    "\n",
    "\n",
    "def benchmark_overview(benchmark_results: dict[str, dict[str, Any]]) -> None:\n",
    "    \"\"\"Generate a table and a dataframe allowing to display the benchmark results.\"\"\"\n",
    "    NB_DISPLAYED_SAMPLES = 10\n",
    "\n",
    "    df_result = pd.DataFrame(benchmark_results).T\n",
    "    df_result[\"short_id\"] = df_result.index.str[:8]\n",
    "    table_result = (\n",
    "        GT(df_result.head(NB_DISPLAYED_SAMPLES), rowname_col=\"short_id\")\n",
    "        .tab_header(\n",
    "            title=\"Recipes ingredients extraction overview\",\n",
    "            subtitle=f\"Accuracy on full dataset: {df_result.correct_inference.mean():.1%}\",\n",
    "        )\n",
    "        .tab_source_note(source_note=f\"Only the first {NB_DISPLAYED_SAMPLES} cases are displayed.\")\n",
    "        .tab_options(\n",
    "            heading_background_color=\"#36665e\",\n",
    "            column_labels_background_color=\"#479487\",\n",
    "        )\n",
    "        .cols_label(recipe_request=\"Request\", computed_result=\"Computed ingredients\", correct_inference=\"Correct\")\n",
    "        .cols_hide(\"expected_result\")\n",
    "        .fmt(lambda x: \"✅\" if x else \"❌\", columns=[\"correct_inference\"])\n",
    "        .opt_align_table_header()\n",
    "        .cols_align(align=\"center\", columns=[\"recipe_request\", \"computed_result\"])\n",
    "        .opt_table_outline()\n",
    "    )\n",
    "\n",
    "    return df_result, table_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the benchmark against a list of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: false\n",
    "\n",
    "\n",
    "LLM_CREDENTIALS = {\"base_url\": \"http://localhost:11434/v1\", \"api_key\": \"ollama\"}\n",
    "AVAILABLE_MODELS = [\"smollm2:135m\", \"smollm2:360m\", \"llama3.2:1b\", \"gemma2:2b\"]\n",
    "\n",
    "models_result = {}\n",
    "for model in AVAILABLE_MODELS:\n",
    "    benchmark_results = benchmark(\n",
    "        test_dataset=test_recipes_requests_ingredients,\n",
    "        llm_credentials=LLM_CREDENTIALS,\n",
    "        llm_model=model,\n",
    "    )\n",
    "    df_result, table_result = benchmark_overview(benchmark_results)\n",
    "\n",
    "    models_result[model] = (df_result, table_result)\n",
    "\n",
    "    print(f\"Model: {model:<15} | Accuracy: {df_result.correct_inference.mean():.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only model that gets 100% accuracy is `gemma2:2b`. It's the biggest model tested, but at 2b it's small enough to \n",
    "have a good inference speed. We will therefore use this model in the recipes inference app.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USED_MODEL = \"gemma2:2b\"\n",
    "models_result[USED_MODEL][1].tab_header(f\"[ {USED_MODEL} ] Recipes ingredients extraction overview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "The current method struggles with imprecise ingredient descriptions. For example, if a user mentions disliking \"meat,\" this general term won't match specific entries like \"beef\" or \"chicken\" in the recipe database. Similarly, a preference against \"spicy food\" won't align with specific spicy ingredients. To address this, we could implement a semantic search to identify the closest matching ingredient in the database and substitute the original term.\n",
    "\n",
    "Additionally, the format of ingredients could be standardized. Currently, some ingredients are listed in plural form while others are singular. To maintain consistency with the recipe database, which uses singular forms, we could convert all ingredients to singular in either a two-step process or a single step using a more advanced model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
